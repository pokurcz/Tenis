{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will substract 3000 rows on beginning\n",
    "data = pd.read_csv(\"atp.csv\")\n",
    "Y = pd.DataFrame(data['target'][3000:]).reset_index(drop=True)\n",
    "X = data.drop(['target'], axis = 1)[3000:].reset_index(drop=True)\n",
    "y = np.asarray(Y).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_std = pd.DataFrame(standard_scaler.fit_transform(X), columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation with chronological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 2711  2712  2713 ... 22708 22709 22710] TEST: [22711 22712 22713 ... 26708 26709 26710]\n",
      "TRAIN: [ 6711  6712  6713 ... 26708 26709 26710] TEST: [26711 26712 26713 ... 30708 30709 30710]\n",
      "TRAIN: [10711 10712 10713 ... 30708 30709 30710] TEST: [30711 30712 30713 ... 34708 34709 34710]\n",
      "TRAIN: [14711 14712 14713 ... 34708 34709 34710] TEST: [34711 34712 34713 ... 38708 38709 38710]\n"
     ]
    }
   ],
   "source": [
    "ts_cv = TimeSeriesSplit(gap=0, max_train_size=20000, n_splits=4, test_size=4000)\n",
    "for train_index, test_index in ts_cv.split(X_std.iloc[:-10000]):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_std.iloc[train_index], X_std.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try raw models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tuning purposes we will change ts_cv for this part of code\n",
    "ts_cv = TimeSeriesSplit(gap=0, max_train_size=15000, n_splits=4, test_size=3000)\n",
    "classifiers = {\n",
    "    'Ridge Classifier' : RidgeClassifier(),\n",
    "    'Random Forest' : RandomForestClassifier(),\n",
    "    'Gradient Boosting' : GradientBoostingClassifier(),\n",
    "    'Logistic Regression' : LogisticRegression(max_iter=1000),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='rmse'),\n",
    "    'Naive Bayes' : GaussianNB(),\n",
    "    'Decision Tree' : DecisionTreeClassifier(),\n",
    "    'k Nearest Neighbors' : KNeighborsClassifier(),\n",
    "    'AdaBoost' : AdaBoostClassifier(),\n",
    "    'Neural Net' :  MLPClassifier(max_iter=1000),\n",
    "    \"QDA\" : QuadraticDiscriminantAnalysis(),\n",
    "    \"SVM\" : SVC(),\n",
    "    #\"Lasso\" : Lasso()\n",
    "}\n",
    "clf_acc = {}\n",
    "clf_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating... Ridge Classifier\n",
      "accuracy 0.7747 with a standard deviation of 0.01\n",
      "Evaluating... Random Forest\n",
      "accuracy 0.7798 with a standard deviation of 0.01\n",
      "Evaluating... Gradient Boosting\n",
      "accuracy 0.8400 with a standard deviation of 0.02\n",
      "Evaluating... Logistic Regression\n",
      "accuracy 0.7747 with a standard deviation of 0.02\n",
      "Evaluating... XGBoost\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in classifiers.items():\n",
    "    print(\"Evaluating...\",name)\n",
    "    scores = cross_val_score(classifier, X_std[-25000:-10000], y[-25000:-10000], cv=ts_cv, n_jobs=-1)\n",
    "    clf_scores[name] = scores\n",
    "    print(f'accuracy {scores.mean():.4f} with a standard deviation of {scores.std():.2f}')\n",
    "    clf_acc[name] = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and intuitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_spectrum(X, search):\n",
    "    print(\"Best n parameter pca : %d (CV mean score=%0.3f):\" % (search.best_estimator_.named_steps[\"pca\"].n_components, search.best_score_))\n",
    "    # Plot the PCA spectrum\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "    \n",
    "    ax0.plot(\n",
    "        np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, \"+\", linewidth=2\n",
    "    )\n",
    "    ax0.set_ylabel(\"PCA explained variance ratio\")\n",
    "\n",
    "    ax0.axvline(\n",
    "        search.best_estimator_.named_steps[\"pca\"].n_components,\n",
    "        linestyle=\":\",\n",
    "        label=\"n_components chosen\",\n",
    "    )\n",
    "    ax0.legend(prop=dict(size=12))\n",
    "\n",
    "    # For each number of components, find the best classifier results\n",
    "    results = pd.DataFrame(search.cv_results_)\n",
    "    components_col = \"param_pca__n_components\"\n",
    "    best_clfs = results.groupby(components_col).apply(\n",
    "        lambda g: g.nlargest(1, \"mean_test_score\")\n",
    "    )\n",
    "\n",
    "    best_clfs.plot(\n",
    "        x=components_col, y=\"mean_test_score\", yerr=\"std_test_score\", legend=False, ax=ax1\n",
    "    )\n",
    "    ax1.plot(\n",
    "        \n",
    "    )\n",
    "    ax1.set_ylabel(\"Classification accuracy (val)\")\n",
    "    ax1.set_xlabel(\"n_components\")\n",
    "\n",
    "    plt.xlim(-1, pca.n_components_ + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [2,5,10,15,20,30,40,50,70,90,110,130,150,180,200,220]\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=2, cv = ts_cv)\n",
    "search.fit(X_std[:-10000], y[:-10000])\n",
    "\n",
    "plot_pca_spectrum(X_std[:-10000], search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "classifier = KNeighborsClassifier()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param_grid = {\n",
    "    \"pca__n_components\" : [5,10,25,40,50,70,100,130,150,180,200,220]\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=2, verbose=2, cv = ts_cv)\n",
    "search.fit(X_std[:-10000], y[:-10000])\n",
    "\n",
    "plot_pca_spectrum(X_std[:-10000], search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "classifier = RidgeClassifier()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [2,5,10,15,20,30,40,50,70,90,110,130,150,180,200,220]\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=2, cv = ts_cv)\n",
    "search.fit(X_std[:-10000], y[:-10000])\n",
    "\n",
    "plot_pca_spectrum(X_std[:-10000], search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree based classifiers are sensitive to data rotation. After experiments, we concluded it would be better not to apply PCA to our Tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_clf = {}\n",
    "best_paramteres = {}\n",
    "hyper_param = {}\n",
    "def tune_hyperparameters(X, Y, param, pipe, n_jobs_=-1, verbose_=2):\n",
    "    search = GridSearchCV(pipe, param, n_jobs=n_jobs_, verbose=verbose_, cv = ts_cv)\n",
    "    search.fit(X,Y)\n",
    "    return search\n",
    "def analyse_params(name, classifier):\n",
    "    print(name, \"tuning... \", end='')\n",
    "    pipe, param,n_jobs = hyper_param[name]\n",
    "    start_time = time.time()\n",
    "    search = tune_hyperparameters(X_std[-25000:-10000], y[-25000:-10000], param, pipe, n_jobs_=n_jobs)\n",
    "    print(f'{time.time() - start_time:.3f} seconds')\n",
    "    print(f\"new score: {search.best_score_:.4f}  |  old score: {clf_acc[name]}  |   params: {search.best_params_}\\n\")\n",
    "    tuned_clf[name] = search.best_estimator_\n",
    "    best_parameters[name] = search.best_params_\n",
    "    #clf_acc[name] = search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge Classifier\n",
    "n_jobs=-1\n",
    "pca = PCA()\n",
    "classifier = RidgeClassifier()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"pca__n_components\": [5,10,30,70,130,140,150,160,180,190,200,210,220,222],\n",
    "    \"classifier__alpha\": [5.0,6.0,7.0,8.0]\n",
    "}\n",
    "hyper_param['Ridge Classifier'] = (pipe, param,n_jobs)\n",
    "analyse_params('Ridge Classifier', classifiers['Ridge Classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "n_jobs=-1\n",
    "classifier = RandomForestClassifier()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"classifier__n_estimators\" : [100],\n",
    "    \"classifier__max_features\" : [30,80,100,130]\n",
    "}\n",
    "hyper_param['Random Forest'] = (pipe, param,n_jobs)\n",
    "analyse_params('Random Forest', classifiers['Random Forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "n_jobs = -1\n",
    "classifier = GradientBoostingClassifier()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"classifier__n_estimators\" : [100],\n",
    "    \"classifier__learning_rate\" : [0.01, 0.1, 0.3],\n",
    "    \"classifier__subsample\" : [0.7, 1.0],\n",
    "    \"classifier__max_depth\" : [3, 7]\n",
    "}\n",
    "hyper_param['Gradient Boosting'] = (pipe, param,n_jobs)\n",
    "analyse_params('Gradient Boosting', classifiers['Gradient Boosting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "n_jobs=-1\n",
    "pca = PCA()\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"pca__n_components\": [70,130,180,200,220,222],\n",
    "    \"classifier__C\" : [10, 1.0, 0.5, 0.4, 0.3, 0.1, 0.05]\n",
    "}\n",
    "hyper_param['Logistic Regression'] = (pipe, param,n_jobs)\n",
    "analyse_params('Logistic Regression', classifiers['Logistic Regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "n_jobs = -1\n",
    "classifier = XGBClassifier(use_label_encoder=False, eval_metric='rmse')\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    'classifier__n_estimators': [100],\n",
    "    'classifier__max_depth': [6],\n",
    "    'classifier__learning_rate': [0.3, 0.4, 0.6],\n",
    "    'classifier__min_child_weight': [0.1, 0.5, 1, 10]\n",
    "}\n",
    "hyper_param['XGBoost'] = (pipe, param,n_jobs)\n",
    "analyse_params('XGBoost', classifiers['XGBoost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "n_jobs=-1\n",
    "pca = PCA()\n",
    "classifier = GaussianNB()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"pca__n_components\": [100, 130, 135, 140, 145, 148, 150, 220],\n",
    "}\n",
    "hyper_param['Naive Bayes'] = (pipe, param,n_jobs)\n",
    "analyse_params('Naive Bayes', classifiers['Naive Bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "n_jobs=-1\n",
    "classifier = DecisionTreeClassifier()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    'classifier__max_depth': [2, 3, 10, 20, 30],\n",
    "    'classifier__min_samples_leaf': [2, 3, 5, 10, 15, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "hyper_param['Decision Tree'] = (pipe, param,n_jobs)\n",
    "analyse_params('Decision Tree', classifiers['Decision Tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Nearest Neighbors\n",
    "n_jobs = 2\n",
    "pca = PCA()\n",
    "classifier = KNeighborsClassifier()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"pca__n_components\": [10,30,70,130,220],\n",
    "    \"classifier__weights\" : ['uniform', 'distance'],\n",
    "    \"classifier__metric\" : ['euclidean', 'manhattan', 'minkowski'],\n",
    "    \"classifier__n_neighbors\" : [2, 5, 10, 20, 50, 80]\n",
    "}\n",
    "hyper_param['k Nearest Neighbors'] = (pipe, param,n_jobs)\n",
    "analyse_params('k Nearest Neighbors', classifiers['k Nearest Neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost\n",
    "n_jobs = -1\n",
    "classifier = AdaBoostClassifier()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    'classifier__n_estimators' : [50,100,500],\n",
    "    'classifier__learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1.0,1.5]\n",
    "}\n",
    "hyper_param['AdaBoost'] = (pipe, param,n_jobs)\n",
    "analyse_params('AdaBoost', classifiers['AdaBoost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Net\n",
    "n_jobs = -1\n",
    "pca = PCA()\n",
    "classifier = MLPClassifier(max_iter=1000)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"pca__n_components\": [222],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__solver': ['sgd', 'adam'],\n",
    "    'classifier__alpha': [0.0001, 0.05],\n",
    "    'classifier__learning_rate': ['constant','adaptive']\n",
    "}\n",
    "hyper_param['Neural Net'] = (pipe, param,n_jobs)\n",
    "analyse_params('Neural Net', classifiers['Neural Net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QDA\n",
    "n_jobs = -1\n",
    "classifier = QuadraticDiscriminantAnalysis()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    'classifier__reg_param': [0, 0.00001, 0.0001, 0.001]\n",
    "}\n",
    "hyper_param['QDA'] = (pipe, param,n_jobs)\n",
    "analyse_params('QDA', classifiers['QDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "n_jobs = -1\n",
    "classifier = SVC()\n",
    "pipe = Pipeline([(\"classifier\", classifier)])\n",
    "param = {\n",
    "    \"classifier__kernel\" : ['poly', 'rbf', 'sigmoid'],\n",
    "    \"classifier__C\" : [50, 10, 1.0, 0.1, 0.01],\n",
    "    \"classifier__gamma\" : ['scale']\n",
    "}\n",
    "hyper_param['SVM'] = (pipe, param,n_jobs)\n",
    "analyse_params('SVM', classifiers['SVM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_cv = TimeSeriesSplit(gap=0, max_train_size=20000, n_splits=4, test_size=4000)\n",
    "clf_acc_tuned = {}\n",
    "clf_scores_tuned = {}\n",
    "for name, classifier in tuned_clf.items():\n",
    "    print(\"Evaluating...\",name)\n",
    "    scores = cross_val_score(classifier, X_std[:-10000], y[:-10000], cv=ts_cv, n_jobs=-1)\n",
    "    clf_scores_tuned[name] = scores\n",
    "    print(f'accuracy {scores.mean():.4f} with a standard deviation of {scores.std():.2f}')\n",
    "    clf_acc_tuned[name] = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved tuned models\n",
    "best_models = {\n",
    "    'Ridge Classifier' : Pipeline(steps=[('pca', PCA(n_components=140)),\n",
    "                ('classifier', RidgeClassifier(alpha=7.0))]),\n",
    "    \n",
    "    'Random Forest' : RandomForestClassifier(max_features=130),\n",
    "    \n",
    "    'Gradient Boosting' : GradientBoostingClassifier(learning_rate=0.3, max_depth=7),\n",
    "    \n",
    "    'Logistic Regression' : Pipeline(steps=[('pca', PCA(n_components=200)),\n",
    "                ('classifier', LogisticRegression(C=0.3, max_iter=1000))]),\n",
    "    \n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='rmse', learning_rate = 0.4, max_depth = 6, min_child_weight = 0.5, n_estimators = 100),\n",
    "    \n",
    "    'Naive Bayes' : Pipeline(steps=[('pca', PCA(n_components=145)), ('classifier', GaussianNB())]),\n",
    "    \n",
    "    'Decision Tree' : DecisionTreeClassifier(criterion='entropy', max_depth=30, min_samples_leaf=5),\n",
    "    \n",
    "    'k Nearest Neighbors' : Pipeline(steps=[('pca', PCA(n_components=220)),\n",
    "                ('classifier',\n",
    "                 KNeighborsClassifier(metric='manhattan', n_neighbors=80,\n",
    "                                      weights='distance'))]),\n",
    "    \n",
    "    'AdaBoost' :  Pipeline(steps=[('classifier', AdaBoostClassifier(n_estimators=500))]),\n",
    "    \n",
    "    'Neural Net' :  Pipeline(steps=[('pca', PCA(n_components=222)),\n",
    "                ('classifier', MLPClassifier(max_iter=1000, activation = 'relu', \n",
    "                                alpha = 0.0001, learning_rate = 'constant', solver = 'adam'))]),\n",
    "    \n",
    "    \"QDA\" : QuadraticDiscriminantAnalysis(reg_param=0),\n",
    "    \n",
    "    \"SVM\" : Pipeline(steps=[('classifier', SVC(C=50))]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlation of models predictions\n",
    "preds = {}\n",
    "for name,clf in tuned_clf.items():\n",
    "    clf.fit(X_std[-20000:-10000], y[-20000:-10000])\n",
    "    pred = clf.predict(X_std[-20000:-10000])\n",
    "    preds[name] = pred\n",
    "preds_df = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,9))\n",
    "plt.matshow(preds_df.corr(), fignum=f.number)\n",
    "plt.xticks(range(0, preds_df.columns.shape[0]), preds_df.columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(0, preds_df.columns.shape[0]), preds_df.columns, fontsize=14)\n",
    "cb = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform 2 stackings:  on all models and the least correlated models.\n",
    "\n",
    "     Ridge Classifier, Logistic Regression, Naive Bayes, KNN, Neural Net, AdaBoost, Decision Tree, XGBoost\n",
    "     \n",
    "We will cross validate Stacking with 4 folds:\n",
    "\n",
    "    20% of fold validation\n",
    "    2/3 of remaining fold for base models training\n",
    "    1/3 of remaining fold for meta model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "for train_index, test_index in ts_cv.split(X_std.iloc[:-10000]):\n",
    "    X_fold = X_std.iloc[np.hstack((train_index, test_index))]\n",
    "    y_fold = y[np.hstack((train_index, test_index))]\n",
    "    folds.append((X_fold, y_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = XGBClassifier(use_label_encoder=False, eval_metric='rmse')\n",
    "base_models_1 = {\n",
    "    'Ridge Classifier' : best_models['Ridge Classifier'],\n",
    "    'Random Forest' : best_models['Random Forest'],\n",
    "    'Gradient Boosting' : best_models['Gradient Boosting'],\n",
    "    'Logistic Regression' : best_models['Logistic Regression'],\n",
    "    'XGBoost': best_models['XGBoost'],\n",
    "    'Naive Bayes' : best_models['Naive Bayes'],\n",
    "    'Decision Tree' : best_models['Decision Tree'],\n",
    "    'k Nearest Neighbors' : best_models['k Nearest Neighbors'],\n",
    "    'AdaBoost' : best_models['AdaBoost'],\n",
    "    'Neural Net' :  best_models['Neural Net'],\n",
    "    \"QDA\" : best_models['QDA'],\n",
    "    \"SVM\" : best_models['SVM'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models_2 = {\n",
    "    'Ridge Classifier' : best_models['Ridge Classifier'],\n",
    "    'Logistic Regression' : best_models['Logistic Regression'],\n",
    "    'XGBoost': best_models['XGBoost'],\n",
    "    'Naive Bayes' : best_models['Naive Bayes'],\n",
    "    'Decision Tree' : best_models['Decision Tree'],\n",
    "    'k Nearest Neighbors' : best_models['k Nearest Neighbors'],\n",
    "    'AdaBoost' : best_models['AdaBoost'],\n",
    "    'Neural Net' :  best_models['Neural Net'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_scoring(X, y, meta_model, base_models, return_meta_args=False):\n",
    "    \n",
    "    # accuracy of models\n",
    "    df_acc = pd.DataFrame(columns=base_models.keys()) # Columns of DF will accord with base_dict keys\n",
    "    \n",
    "    # model predictions for the meta model\n",
    "    train_predictions = {}\n",
    "    val_predictions = {}\n",
    "    \n",
    "    # Split the data length into 2/3 base-training data and 1/3 meta-training data\n",
    "    \n",
    "    n_valid = round(X.shape[0] * 0.2)\n",
    "    n_models = X.shape[0] - n_valid # n_valid is given as per above\n",
    "    n_meta = round(n_models/3) # data size for meta model\n",
    "    n_base = n_models - n_meta # data size for base models\n",
    "    \n",
    "    X_base = X.iloc[:n_base,:]\n",
    "    X_meta = X.iloc[n_base:(n_base + n_meta),:]\n",
    "    X_valid = X.iloc[n_models:(n_models + n_valid),:]\n",
    "    \n",
    "    y_base = y[:n_base]\n",
    "    y_meta = y[n_base:(n_base + n_meta)]\n",
    "    y_valid = y[n_models:(n_models + n_valid)]\n",
    "    \n",
    "    # Fit the base models to the base-training set, and generate predictions\n",
    "    for name, clf in base_models.items():\n",
    "        clf.fit(X_base, y_base)\n",
    "        \n",
    "        pred_meta = list(clf.predict(X_meta)) # Generate predictions on meta-training set\n",
    "        train_predictions[name] = pred_meta # Append predictions to dictionary\n",
    "        \n",
    "        pred_valid = list(clf.predict(X_valid)) # Generate predictions on validation set for meta model\n",
    "        val_predictions[name] = pred_valid\n",
    "\n",
    "        df_acc[name] = pd.Series(round(accuracy_score(pred_valid, y_valid), 5)) # save score of classifier\n",
    "    \n",
    "    # Transform dictionary of predictions for meta model into DataFrames        \n",
    "    df_meta_train = pd.DataFrame(train_predictions)\n",
    "    df_meta_valid = pd.DataFrame(val_predictions)\n",
    "    \n",
    "    # Train meta model using base models' predictions of meta-training set\n",
    "    meta_model.fit(df_meta_train, y_meta)\n",
    "    \n",
    "    # Generate meta model predictions of validation set\n",
    "    meta_predictions = meta_model.predict(df_meta_valid)            \n",
    "    \n",
    "    df_acc['Stack Model'] = pd.Series(round(accuracy_score(meta_predictions, y_valid), 5)) # save score of Stack Model\n",
    "    if return_meta_args:\n",
    "        return df_acc, df_meta_train, y_meta, df_meta_valid, y_valid\n",
    "    return df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_res_1 = []\n",
    "for i,f in enumerate(folds):\n",
    "    print(f'Evaluating fold {i + 1}...')\n",
    "    folds_res_1.append(fold_scoring(*f, meta_model, base_models_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_1 = pd.DataFrame(pd.concat(folds_res_1).mean(), columns = ['CV accuracy'])\n",
    "acc_df_1['Classifier'] = acc_df_1.index\n",
    "acc_df_1 = acc_df_1.sort_values(by=['CV accuracy'])\n",
    "acc_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cm.get_cmap('viridis')\n",
    "colors = v(np.linspace(0, 1, acc_df_1.shape[0] + 4))\n",
    "fig, ax0 = plt.subplots(figsize=(15, 10))\n",
    "acc_df_1.plot.bar(x = 'Classifier', y='CV accuracy', ax = ax0, color = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_res_2 = []\n",
    "for i,f in enumerate(folds):\n",
    "    print(f'Evaluating fold {i + 1}...')\n",
    "    folds_res_2.append(fold_scoring(*f, meta_model, base_models_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_2 = pd.DataFrame(pd.concat(folds_res_2).mean(), columns = ['CV accuracy'])\n",
    "acc_df_2['Classifier'] = acc_df_2.index\n",
    "acc_df_2 = acc_df_2.sort_values(by=['CV accuracy'])\n",
    "acc_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cm.get_cmap('PuBuGn_r')\n",
    "colors = v(np.linspace(0, 1, acc_df_2.shape[0] + 4))\n",
    "fig, ax0 = plt.subplots(figsize=(15, 10))\n",
    "acc_df_2.plot.bar(x = 'Classifier', y='CV accuracy', ax = ax0, color = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
